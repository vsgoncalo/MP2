{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       What are the most relevant actors in movie_key? \n",
      "1                        What actors entered movie_key? \n",
      "2             Which characters were there on movie_key? \n",
      "3             What characters can be seen in movie_key? \n",
      "4           What are the languages spoken in movie_key? \n",
      "5                    Who was the director of movie_key? \n",
      "6                             Who starred in movie_key? \n",
      "7      What actor played the part of char_key in movi...\n",
      "8                      Who entered the movie movie_key? \n",
      "9      What actors played a part in movie_key char_key? \n",
      "10                    What actors starred in movie_key? \n",
      "11               What were the characters in movie_key? \n",
      "12                What characters entered in movie_key? \n",
      "13     What were the parts played in the movie char_k...\n",
      "14             What actor played char_key in movie_key? \n",
      "15     Who has played the part of char_keyBond in mov...\n",
      "16     What was the character that people_key played ...\n",
      "17     What was the part played in movie_key by peopl...\n",
      "18     What was the role taken by people_key in the m...\n",
      "19          What role did people_key play in movie_key? \n",
      "20     Tell me the role played by people_key in movie...\n",
      "21     What was the role played by people_key in movi...\n",
      "22     Who played the part of movie_keydy in movie_key? \n",
      "23          Who voiced char_key in the movie movie_key? \n",
      "24     What actor played the role of char_key in movi...\n",
      "25     What are the actors that participated with peo...\n",
      "26              Who acted with people_key in movie_key? \n",
      "27       Who acted with movie_key Roberts in movie_key? \n",
      "28     Which actors participated in movie_key along s...\n",
      "29              Who acted with people_key in movie_key? \n",
      "                             ...                        \n",
      "178    Tell me the title of shortest movie ever relea...\n",
      "179    From all the movies made, what's the shortest ...\n",
      "180              Which movie has the shortest run time? \n",
      "181                 Which movie has the highest rating? \n",
      "182              Which movie was given the best rating? \n",
      "183                       What is the best rated movie? \n",
      "184            In average, what's the best rated movie? \n",
      "185          What movie received the best vote average? \n",
      "186    What was the movie that received the worst rev...\n",
      "187    Tell me what's the movie with the lowest rating. \n",
      "188    Out of all movies produced, what is the one wi...\n",
      "189    In average, what movie has received the worst ...\n",
      "190             Which movie had the worst vote average? \n",
      "191              How many companies produced movie_key? \n",
      "192    How many companies were involved in the produc...\n",
      "193    The movie movie_key was produced by how many c...\n",
      "194    The production of movie_key involved how many ...\n",
      "195       movie_key was produced by how many companies? \n",
      "196           In how many countries was movie_key shot? \n",
      "197           movie_key was shot in how many countries? \n",
      "198    The movie movie_key was produced in how many c...\n",
      "199    The production of movie_key was made in how ma...\n",
      "200    What's the char_key of countries where movie_k...\n",
      "201    How many different languages are spoken in mov...\n",
      "202    During the movie movie_key, how many languages...\n",
      "203       How many languages can be heard in movie_key? \n",
      "204    On the movie movie_key, how many idioms can be...\n",
      "205    What's the char_key of spoken languages in mov...\n",
      "206    In how many movies did people_key play a part ...\n",
      "207     people_key has participated in how many movies? \n",
      "Name: 0, Length: 208, dtype: object\n",
      "0     Which are the most relevant actors in movie_key? \n",
      "1            Which characters were there on movie_key? \n",
      "2     What characters can be seen in the movie movie...\n",
      "3     What are the languages that are spoken in movi...\n",
      "4            Who was the famous director of movie_key? \n",
      "5     Which are the most relevant actors in Great ch...\n",
      "6            Which characters were there on movie_key? \n",
      "7     What characters can be seen in the movie movie...\n",
      "8     What are the languages that are spoken in movi...\n",
      "9            Who was the famous director of movie_key? \n",
      "10    Which are the most important guys acting in mo...\n",
      "11    Which are the most important actors from movie...\n",
      "12      Who are the most relevant actors in movie_key? \n",
      "13              Which are the actors playing movie_key?\n",
      "14            Which were the characters from movie_key?\n",
      "15          What characters can be found in movie_key? \n",
      "16          What are the languages spoken in movie_key?\n",
      "17            Who was the famous director of movie_key?\n",
      "18                   Who was the director of movie_key?\n",
      "19                          Who directed the movie_key?\n",
      "20    Which are the most important actors from movie...\n",
      "21       Who are the most relevant actors in movie_key?\n",
      "22              Which are the actors playing movie_key?\n",
      "23           Which were the characters from movie_key? \n",
      "24          What characters can be found in movie_key? \n",
      "25                What are the languages in movie_key? \n",
      "26           Who was the famous director of movie_key? \n",
      "27                   Who was the director of movie_key?\n",
      "28                              Who directed movie_key?\n",
      "29             Which are the main actors in movie_key? \n",
      "30         Which are the main characters in movie_key? \n",
      "31    What characters can there in the movie movie_k...\n",
      "32            Which languages are spoken in movie_key? \n",
      "33            Who was the guy that directed movie_key? \n",
      "34    Which are the actors with the main role in mov...\n",
      "35                     Which are movie_key characters? \n",
      "36                What are the languages in movie_key? \n",
      "37    Who was the director, very famous, that direct...\n",
      "38    Which are the most important guys acting in mo...\n",
      "39    Which are the most relevant actors from movie_...\n",
      "40             Who are the actors playing in movie_key?\n",
      "41    What are the names of the main characters from...\n",
      "Name: 0, dtype: object\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "from nltk import FreqDist, pos_tag, ne_chunk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.classify import accuracy\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "df = pd.read_table('QuestoesConhecidas.txt', header=None, encoding = 'utf-8')\n",
    "nq = pd.read_table('NovasQuestoes.txt', header=None, encoding = 'utf-8')\n",
    "nqr = pd.read_table('NovasQuestoesResultados.txt', header=None, encoding = 'utf-8')\n",
    "char = pd.read_table('recursos/list_characters.txt', sep=\"\\n\", header=None, encoding = 'utf-8')\n",
    "company = pd.read_table('recursos/list_companies.txt', header=None, encoding = 'utf-8')\n",
    "genre = pd.read_table('recursos/list_genres.txt', header=None, encoding = 'utf-8')\n",
    "job = pd.read_table('recursos/list_jobs.txt', header=None, encoding = 'utf-8')\n",
    "keyword = pd.read_table('recursos/list_keywords.txt', header=None, encoding = 'utf-8')\n",
    "movie = pd.read_table('recursos/list_movies.txt', header=None, encoding = 'utf-8')\n",
    "people = pd.read_table('recursos/list_people.txt', sep=\"\\n\", header=None, encoding = 'utf-8')\n",
    "\n",
    "\n",
    "Y = df[0]\n",
    "X = df[1]\n",
    "nq = nq[0]\n",
    "nqr = nqr[0]\n",
    "\n",
    "classes = []\n",
    "all_words = []\n",
    "people_set = []\n",
    "movie_set = []\n",
    "#key_list = [char_list, company_list, genre_list, job_list, keyword_list,\n",
    " #           movie_list, people_list]\n",
    "\n",
    "\n",
    "for word in people[0]:\n",
    "   #print(word)\n",
    "    people_set.append(word)\n",
    "people_set = set(people_set)\n",
    "\n",
    "for word in movie[0]:\n",
    "    #print(word)\n",
    "    movie_set.append(word)\n",
    "movie_set = set(movie_set)\n",
    "\n",
    "    \n",
    "key_dict = {\"movie_key\":movie_set, \"people_key\":people_set}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "def remove_spaces(text):\n",
    "    return text.strip(' ')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_text = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            filtered_text.append(word)\n",
    "    return filtered_text\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def gather_all_words(text):\n",
    "    for words in text:\n",
    "        words = word_tokenize(words)\n",
    "        for word in words:\n",
    "            all_words.append(word)\n",
    "    return all_words\n",
    "\n",
    "def find_features(text):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in text)\n",
    "    return features\n",
    "\n",
    "def find_feature_set(data):\n",
    "    feature_set = []\n",
    "    for text, clss in data:\n",
    "        feature_set.append((find_features(text),clss))\n",
    "    return feature_set\n",
    "\n",
    "def replace_keyword(data):\n",
    "    data = data.tolist()\n",
    "    text = '<separador>'.join(data)\n",
    "    for name, key in key_dict.items():\n",
    "        for val in key:\n",
    "            if \"*\" in val:\n",
    "                val = val.replace('*','\\*')\n",
    "            if re.search('\\\\b' + val + '\\\\b', text):\n",
    "                text = text.replace(val, name)\n",
    "    data = text.split('<separador>')\n",
    "    data = pd.DataFrame(data)\n",
    "    return data\n",
    "\n",
    "for _, item in Y.iteritems():\n",
    "    if item not in classes:\n",
    "        classes.append(item)\n",
    "                \n",
    "classes = list(map(remove_spaces, classes))\n",
    "#print(classes)\n",
    "\n",
    "plt.figure()\n",
    "df.groupby(0).count().plot.bar(ylim=0)\n",
    "plt.show()\n",
    "\n",
    "X = replace_keyword(X)[0]\n",
    "print(X)\n",
    "X = X.apply(lambda text : clean_text(text))\n",
    "#print(X.head())\n",
    "X = X.apply(lambda text : word_tokenize(text))\n",
    "#print(X.head())\n",
    "X = X.apply(lambda text : remove_stopwords(text))\n",
    "#print(X.head())\n",
    "X = X.apply(lambda text : lemmatize_words(text))\n",
    "#print(X.head())\n",
    "\n",
    "gather_all_words(X)\n",
    "        \n",
    "all_words_fd = FreqDist(all_words)\n",
    "        \n",
    "#print(all_words)\n",
    "#print(all_words_fd)\n",
    "\n",
    "#print(X)\n",
    "\n",
    "word_features = list(all_words_fd.keys())\n",
    "#print(word_features)\n",
    "\n",
    "#features = find_features(X[0])\n",
    "#print(features)\n",
    "\n",
    "quest = list(zip(X, Y))\n",
    "\n",
    "np.random.seed = 1       #seed para repetição\n",
    "np.random.shuffle(quest) #shuffling para garantir que as classes não ficam agrupadas\n",
    "\n",
    "feature_set = find_feature_set(quest)\n",
    "#print(feature_set)\n",
    "#print(quest)\n",
    "\n",
    "nq = replace_keyword(nq)[0]\n",
    "print(nq)\n",
    "nq = nq.apply(lambda text : clean_text(text))\n",
    "#print(X.head())\n",
    "nq = nq.apply(lambda text : word_tokenize(text))\n",
    "#print(X.head())\n",
    "nq = nq.apply(lambda text : remove_stopwords(text))\n",
    "#print(X.head())\n",
    "nq = nq.apply(lambda text : lemmatize_words(text))\n",
    "#print(nqr)\n",
    "\n",
    "nq_feature_set = []\n",
    "for text in nq:\n",
    "    nq_feature_set.append(find_features(text))\n",
    "    \n",
    "#print(nq_feature_set)\n",
    "\n",
    "test = list(zip(nq, nqr))\n",
    "\n",
    "test_feature_set = find_feature_set(test)\n",
    "#print(test_feature_set)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors accuracy : 85.71428571428571\n",
      "Decision Tree accuracy : 88.09523809523809\n",
      "Random Forest accuracy : 85.71428571428571\n",
      "Logistic Regression accuracy : 90.47619047619048\n",
      "SGD Classifier accuracy : 85.71428571428571\n",
      "Naive Bayes accuracy : 83.33333333333334\n",
      "SVM Linear accuracy : 88.09523809523809\n"
     ]
    }
   ],
   "source": [
    "names = ['K Nearest Neighbors','Decision Tree','Random Forest','Logistic Regression','SGD Classifier', 'Naive Bayes', 'SVM Linear']\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    \n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "for name, classifier in models:\n",
    "    nltk_classifier = SklearnClassifier(classifier)\n",
    "    nltk_classifier.train(feature_set)\n",
    "    class_set = nltk_classifier.classify_many(nq_feature_set)\n",
    "    accuracy = nltk.classify.accuracy(nltk_classifier, test_feature_set) * 100\n",
    "    print(name, \"accuracy :\", accuracy)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '?', 'Fine', '?', 'I', 'good', 'well', '.']\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Hello how are you doing? Fine and you? I am good as well.\"\n",
    "\n",
    "words = word_tokenize(example_text)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        \n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goose\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"geese\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Utilizador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Utilizador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Utilizador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Utilizador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Utilizador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Utilizador\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quest_train, quest_test, class_train, class_test = train_test_split(questoes, classes, test_size=0.3, random_state=0)\n",
    "\n",
    "print(classes.value_counts())\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where are movie you trying to find\n"
     ]
    }
   ],
   "source": [
    "cities = ['London', 'age of ultron', 'Birmingham']\n",
    "where = \"Where are age of ultron you trying to find\"\n",
    "for city in cities:\n",
    "    if city in where:\n",
    "        where = where.replace(city, \"movie\")\n",
    "        print(where)\n",
    "        break\n",
    "else:\n",
    "    print(\"I'm not to sure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boy'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"boys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': True, 'n': True, 'c': True, 'o': True, 's': False, 'm': False, 'i': False, '_': False, 'k': False, 'y': True, '?': False, 'd': True, 'f': False, 'g': False, 'u': False, 'p': False, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': False, 'n': True, 'c': True, 'o': True, 's': False, 'm': False, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': True, 'g': True, 'u': False, 'p': False, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': False, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': True, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': False, 'f': False, 'g': False, 'u': False, 'p': False, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': False, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': False, 'r': False, 'e': True, 'l': True, 'v': True, 'n': True, 'c': True, 'o': True, 's': True, 'm': False, 'i': False, '_': False, 'k': True, 'y': False, '?': False, 'd': False, 'f': False, 'g': True, 'u': True, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': False, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': False, 'n': False, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': True, '?': False, 'd': True, 'f': True, 'g': True, 'u': True, 'p': False, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': True, 'n': True, 'c': True, 'o': True, 's': True, 'm': False, 'i': False, '_': False, 'k': False, 'y': True, '?': False, 'd': False, 'f': False, 'g': True, 'u': False, 'p': False, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': False, 'n': True, 'c': True, 'o': False, 's': True, 'm': False, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': False, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': True, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': False, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': False, 'i': True, '_': False, 'k': True, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': True, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': True, 'g': True, 'u': True, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': True, '?': False, 'd': True, 'f': False, 'g': True, 'u': True, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': False, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': True, 'n': True, 'c': True, 'o': True, 's': True, 'm': False, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': False, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': False, 'i': True, '_': False, 'k': False, 'y': True, '?': False, 'd': True, 'f': False, 'g': True, 'u': False, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': False, 'n': True, 'c': True, 'o': False, 's': True, 'm': False, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': False, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': False, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': True, 'g': True, 'u': True, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': False, 'i': True, '_': False, 'k': True, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': True, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': True, 'g': True, 'u': True, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': False, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': False, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': False, 'n': True, 'c': True, 'o': False, 's': True, 'm': False, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': False, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': False, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': True, 'n': True, 'c': True, 'o': True, 's': True, 'm': False, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': False, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': False, 'i': True, '_': False, 'k': False, 'y': True, '?': False, 'd': True, 'f': False, 'g': True, 'u': False, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': False, 'n': True, 'c': True, 'o': False, 's': True, 'm': False, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': False, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': False, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': True, 'g': True, 'u': True, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': False, 'n': True, 'c': True, 'o': False, 's': True, 'm': False, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': True, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': True, 'g': True, 'u': True, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': False, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': False, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': False, 'a': True, 't': True, 'r': True, 'e': True, 'l': False, 'v': False, 'n': True, 'c': True, 'o': False, 's': True, 'm': False, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': False, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': True, 'p': False, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': True, 'p': False, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': True, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': True, 'p': False, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': False, 'e': True, 'l': True, 'v': False, 'n': True, 'c': False, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': True, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': True, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': True, '?': False, 'd': True, 'f': False, 'g': True, 'u': True, 'p': False, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': True, 'p': False, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': True, 'p': False, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': False, 'e': True, 'l': True, 'v': False, 'n': True, 'c': False, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': True, 'p': False, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': True, 'g': True, 'u': True, 'p': False, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': True, '?': False, 'd': True, 'f': False, 'g': True, 'u': True, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': True, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': True, 'p': False, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': True, '?': False, 'd': True, 'f': False, 'g': True, 'u': True, 'p': True, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}, {'W': False, 'h': True, 'a': True, 't': True, 'r': True, 'e': True, 'l': True, 'v': False, 'n': True, 'c': True, 'o': True, 's': True, 'm': True, 'i': True, '_': False, 'k': False, 'y': False, '?': False, 'd': True, 'f': False, 'g': True, 'u': True, 'p': False, \"'\": False, 'E': False, 'A': False, 'T': False, 'L': False, ':': False, 'F': False, 'w': False, '.': False, 'S': False, 'b': True, 'z': False, 'I': False, 'x': False, ',': False, '0': False, 'K': False}]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'movies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f65895d46d28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmovies_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmovies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmov_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmov\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmovies_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmov_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmov\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'movies' is not defined"
     ]
    }
   ],
   "source": [
    "movies_list = movies[0]\n",
    "mov_list = []\n",
    "\n",
    "for mov in movies_list:\n",
    "    mov_list.append(mov)\n",
    "    \n",
    "print(replace_keyword(\"I watched Avengers: Age of Ultron and I loved it.\", mov_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(bool(re.match(\"^[a-zA-Z]+(([',. -][a-zA-Z ])?[a-zA-Z]*)*$\",\"Joao pedro dos sa2ntos\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(11, 19), match='Robinson'>\n"
     ]
    }
   ],
   "source": [
    "val = \"Robinson\"\n",
    "text = \"My name is Robinson James\"\n",
    "print(re.search(r'\\b{}\\b'.format(val),text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\bWhat the #$*! Do We (K).now!?\\\\b'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(r'\\b{}\\b'.format(\"What the #$*! Do We (K).now!?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fa0134f942c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versão para entregar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk import FreqDist, pos_tag, ne_chunk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, PunktSentenceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.classify import accuracy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "df = pd.read_table(\"QuestoesConhecidas.txt\", header=None, encoding = 'utf-8')\n",
    "nq = pd.read_table(\"NovasQuestoes.txt\", header=None, encoding = 'utf-8')\n",
    "char_list = pd.read_table('recursos/list_characters.txt', sep=\"\\n\", header=None, encoding = 'utf-8')\n",
    "company_list = pd.read_table('recursos/list_companies.txt', header=None, encoding = 'utf-8')\n",
    "genre_list = pd.read_table('recursos/list_genres.txt', header=None, encoding = 'utf-8')\n",
    "job_list = pd.read_table('recursos/list_jobs.txt', header=None, encoding = 'utf-8')\n",
    "keyword_list = pd.read_table('recursos/list_keywords.txt', header=None, encoding = 'utf-8')\n",
    "movie_list = pd.read_table('recursos/list_movies.txt', header=None, encoding = 'utf-8')\n",
    "people_list = pd.read_table('recursos/list_people.txt', sep=\"\\n\", header=None, encoding = 'utf-8')\n",
    "\n",
    "\n",
    "Y = df[0]\n",
    "X = df[1]\n",
    "nq = nq[0]\n",
    "\n",
    "all_words = []\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "def remove_spaces(text):\n",
    "    return text.strip(' ')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_text = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            filtered_text.append(word)\n",
    "    return filtered_text\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def gather_all_words(text):\n",
    "    for words in text:\n",
    "        words = word_tokenize(words)\n",
    "        for word in words:\n",
    "            all_words.append(word)\n",
    "    return all_words\n",
    "\n",
    "def find_features(text):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in text)\n",
    "    return features\n",
    "\n",
    "def find_feature_set(data):\n",
    "    feature_set = []\n",
    "    for text, clss in data:\n",
    "        feature_set.append((find_features(text),clss))\n",
    "    return feature_set\n",
    "\n",
    "X = X.apply(lambda text : clean_text(text))\n",
    "X = X.apply(lambda text : word_tokenize(text))\n",
    "X = X.apply(lambda text : remove_stopwords(text))\n",
    "X = X.apply(lambda text : lemmatize_words(text))\n",
    "\n",
    "gather_all_words(X)\n",
    "        \n",
    "all_words_fd = FreqDist(all_words)\n",
    "word_features = list(all_words_fd.keys())\n",
    "quest = list(zip(X, Y))\n",
    "\n",
    "np.random.seed = 1       #seed para repetição\n",
    "np.random.shuffle(quest) #shuffling para garantir que as classes não ficam agrupadas\n",
    "\n",
    "feature_set = find_feature_set(quest)\n",
    "\n",
    "nq = nq.apply(lambda text : clean_text(text))\n",
    "nq = nq.apply(lambda text : word_tokenize(text))\n",
    "nq = nq.apply(lambda text : remove_stopwords(text))\n",
    "nq = nq.apply(lambda text : lemmatize_words(text))\n",
    "\n",
    "nq_feature_set = []\n",
    "for text in nq:\n",
    "    nq_feature_set.append(find_features(text))\n",
    "\n",
    "nltk_classifier = SklearnClassifier(LogisticRegression())\n",
    "nltk_classifier.train(feature_set)\n",
    "class_set = nltk_classifier.classify_many(nq_feature_set)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actor_name ', 'character_name ', 'character_name ', 'spoken_language ', 'person_name ', 'actor_name ', 'character_name ', 'character_name ', 'spoken_language ', 'person_name ', 'actor_name ', 'actor_name ', 'actor_name ', 'actor_name ', 'character_name ', 'character_name ', 'spoken_language ', 'person_name ', 'person_name ', 'person_name ', 'actor_name ', 'actor_name ', 'actor_name ', 'character_name ', 'character_name ', 'spoken_language ', 'person_name ', 'person_name ', 'person_name ', 'actor_name ', 'character_name ', 'character_name ', 'spoken_language ', 'person_name ', 'actor_name ', 'character_name ', 'spoken_language ', 'person_name ', 'original_title ', 'actor_name ', 'actor_name ', 'character_name ']\n"
     ]
    }
   ],
   "source": [
    "print(class_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(len([1,2,3])):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
